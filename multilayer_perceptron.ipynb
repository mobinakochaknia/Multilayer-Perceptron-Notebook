{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8KGbM_biRpY"
      },
      "source": [
        "<b><h4>HW3 - Notebook: Multi-Layer perceptron</h4></b>\n",
        "\n",
        "[CE40477: Machine Learning](https://www.sharifml.ir/)\n",
        "\n",
        "__Course Instructor__: Dr. Sharifi-Zarchi\n",
        "\n",
        "__Notebook Authors__: Amir Ezzati & Ali Bavafa\n",
        "\n",
        "Name: mobina kochaknia                \n",
        "Student-ID:401106396"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3D-dKQK9RSV"
      },
      "source": [
        "# Import & Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srjf6Gd38-uP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mi1fqVs9jOfx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gdown\n",
        "from sklearn.datasets import fetch_openml\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import random\n",
        "\n",
        "from itertools import combinations_with_replacement\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xc-dSl8s6Jz",
        "outputId": "43d0a952-0daa-451f-925f-e583becd71d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed 2024 has been set.\n"
          ]
        }
      ],
      "source": [
        "def seed_setter(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Function that controls randomness.\n",
        "\n",
        "  Args:\n",
        "    seed : Integer\n",
        "      A non-negative integer that defines the random state.\n",
        "    seed_torch : Boolean\n",
        "      If `True` sets the random seed for pytorch tensors.\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)\n",
        "\n",
        "\n",
        "def train_test_classification(net, criterion, optimizer, train_loader,\n",
        "                              test_loader, num_epochs=1, verbose=True,\n",
        "                              training_plot=False):\n",
        "  \"\"\"\n",
        "  Accumulate training loss/Evaluate performance\n",
        "\n",
        "  Args:\n",
        "    net: Instance of Net class\n",
        "      Describes the model with ReLU activation, batch size 128\n",
        "    criterion: torch.nn type\n",
        "      Criterion combines LogSoftmax and NLLLoss in one single class.\n",
        "    optimizer: torch.optim type\n",
        "      Implements Adam algorithm.\n",
        "    train_loader: torch.utils.data type\n",
        "      Combines the train dataset and sampler, and provides an iterable over the given dataset.\n",
        "    test_loader: torch.utils.data type\n",
        "      Combines the test dataset and sampler, and provides an iterable over the given dataset.\n",
        "    num_epochs: int\n",
        "      Number of epochs [default: 1]\n",
        "    verbose: boolean\n",
        "      If True, print statistics\n",
        "    training_plot=False\n",
        "      If True, display training plot\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  net.train()\n",
        "  train_losses = []\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      # Get the inputs; data is a list of [inputs, labels]\n",
        "      inputs, labels = data\n",
        "      inputs = inputs.float()\n",
        "      labels = labels.long()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if verbose:\n",
        "        train_losses += [loss.item()]\n",
        "\n",
        "  net.eval()\n",
        "\n",
        "  def test(data_loader):\n",
        "    \"\"\"\n",
        "    Function to gauge network performance\n",
        "\n",
        "    Args:\n",
        "      data_loader: torch.utils.data type\n",
        "        Combines the test dataset and sampler, and provides an iterable over the given dataset.\n",
        "\n",
        "    Returns:\n",
        "      acc: float\n",
        "        Performance of the network\n",
        "      total: int\n",
        "        Number of datapoints in the dataloader\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in data_loader:\n",
        "      inputs, labels = data\n",
        "      inputs = inputs.float()\n",
        "      labels = labels.long()\n",
        "\n",
        "      outputs = net(inputs)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    return total, acc\n",
        "\n",
        "  train_total, train_acc = test(train_loader)\n",
        "  test_total, test_acc = test(test_loader)\n",
        "\n",
        "  if verbose:\n",
        "    print(f'\\nAccuracy on the {train_total} training samples: {train_acc:0.2f}')\n",
        "    print(f'Accuracy on the {test_total} testing samples: {test_acc:0.2f}\\n')\n",
        "\n",
        "  if training_plot:\n",
        "    plt.plot(train_losses)\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Training loss')\n",
        "    plt.show()\n",
        "\n",
        "  return train_acc, test_acc\n",
        "\n",
        "\n",
        "def shuffle_and_split_data(X, y, seed):\n",
        "  \"\"\"\n",
        "  Helper function to shuffle and split data\n",
        "\n",
        "  Args:\n",
        "    X: torch.tensor\n",
        "      Input data\n",
        "    y: torch.tensor\n",
        "      Corresponding target variables\n",
        "    seed: int\n",
        "      Set seed for reproducibility\n",
        "\n",
        "  Returns:\n",
        "    X_test: torch.tensor\n",
        "      Test data [20% of X]\n",
        "    y_test: torch.tensor\n",
        "      Labels corresponding to above mentioned test data\n",
        "    X_train: torch.tensor\n",
        "      Train data [80% of X]\n",
        "    y_train: torch.tensor\n",
        "      Labels corresponding to above mentioned train data\n",
        "  \"\"\"\n",
        "  # Set seed for reproducibility\n",
        "  torch.manual_seed(seed)\n",
        "  # Number of samples\n",
        "  N = X.shape[0]\n",
        "  # Shuffle data\n",
        "  shuffled_indices = torch.randperm(N)\n",
        "  X = X[shuffled_indices]\n",
        "  y = y[shuffled_indices]\n",
        "\n",
        "  test_size = int(0.2 * N)\n",
        "  X_test = X[:test_size]\n",
        "  y_test = y[:test_size]\n",
        "  X_train = X[test_size:]\n",
        "  y_train = y[test_size:]\n",
        "\n",
        "  return X_test, y_test, X_train, y_train\n",
        "\n",
        "\n",
        "def sample_grid(M=500, x_max=2.0):\n",
        "  \"\"\"\n",
        "  Helper function to simulate sample meshgrid\n",
        "\n",
        "  Args:\n",
        "    M: int\n",
        "      Size of the constructed tensor with meshgrid\n",
        "    x_max: float\n",
        "      Defines range for the set of points\n",
        "\n",
        "  Returns:\n",
        "    X_all: torch.tensor\n",
        "      Concatenated meshgrid tensor\n",
        "  \"\"\"\n",
        "  ii, jj = torch.meshgrid(torch.linspace(-x_max, x_max,M),\n",
        "                          torch.linspace(-x_max, x_max, M),\n",
        "                          indexing='ij')\n",
        "  X_all = torch.cat([ii.unsqueeze(-1),\n",
        "                     jj.unsqueeze(-1)],\n",
        "                     dim=-1).view(-1, 2)\n",
        "  return X_all\n",
        "\n",
        "\n",
        "def plot_decision_map(X_all, y_pred, X_test, y_test,\n",
        "                      M=500, x_max=2.0, eps=1e-3):\n",
        "  \"\"\"\n",
        "  Helper function to plot decision map\n",
        "\n",
        "  Args:\n",
        "    X_all: torch.tensor\n",
        "      Concatenated meshgrid tensor\n",
        "    y_pred: torch.tensor\n",
        "      Labels predicted by the network\n",
        "    X_test: torch.tensor\n",
        "      Test data\n",
        "    y_test: torch.tensor\n",
        "      Labels of the test data\n",
        "    M: int\n",
        "      Size of the constructed tensor with meshgrid\n",
        "    x_max: float\n",
        "      Defines range for the set of points\n",
        "    eps: float\n",
        "      Decision threshold\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  decision_map = torch.argmax(y_pred, dim=1)\n",
        "\n",
        "  for i in range(len(X_test)):\n",
        "    indeces = (X_all[:, 0] - X_test[i, 0])**2 + (X_all[:, 1] - X_test[i, 1])**2 < eps\n",
        "    decision_map[indeces] = (K + y_test[i]).long()\n",
        "\n",
        "  decision_map = decision_map.view(M, M).cpu()\n",
        "  plt.imshow(decision_map, extent=[-x_max, x_max, -x_max, x_max], cmap='jet')\n",
        "  plt.axis('off')\n",
        "  plt.plot()\n",
        "\n",
        "\n",
        "def plot_function_apx(x_vals, relu_activations, predicted_output):\n",
        "    \"\"\"\n",
        "    Visualizes ReLU activations and the resulting function approximation.\n",
        "\n",
        "    Args:\n",
        "      x_vals: torch.tensor\n",
        "        Input x-axis data points.\n",
        "      relu_activations: torch.tensor\n",
        "        Calculated ReLU activations along x-axis for each ReLU unit.\n",
        "      predicted_output: torch.tensor\n",
        "        Estimated output labels or function approximations (weighted sum of ReLUs).\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(8, 6))\n",
        "\n",
        "    ## Plot ReLU activations\n",
        "    axes[0].plot(x_vals, relu_activations.T)\n",
        "    axes[0].set_xlabel('x-values', fontsize=12)\n",
        "    axes[0].set_ylabel('Activation Levels', fontsize=12)\n",
        "    axes[0].set_title('ReLU Activations (Basis Functions)', fontsize=14)\n",
        "    relu_labels = [f\"ReLU {i + 1}\" for i in range(relu_activations.shape[0])]\n",
        "    axes[0].legend(relu_labels, ncol=2)\n",
        "\n",
        "    ## Plot the function approximation and the ground truth\n",
        "    axes[1].plot(x_vals, torch.sin(x_vals), label='Ground Truth (sin(x))', color='g', linewidth=2)\n",
        "    axes[1].plot(x_vals, predicted_output, label='Predicted Output', linestyle='--', color='r')\n",
        "    axes[1].set_xlabel('x-values', fontsize=12)\n",
        "    axes[1].set_ylabel('y(x)', fontsize=12)\n",
        "    axes[1].set_title('Function Approximation vs Ground Truth', fontsize=14)\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout(pad=2)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_images(image, num_row=2, num_col=5):\n",
        "    # plot images\n",
        "    image_size = int(np.sqrt(image.shape[-1]))\n",
        "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
        "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
        "    for i in range(num_row*num_col):\n",
        "        ax = axes[i//num_col, i%num_col]\n",
        "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "SEED = 2024\n",
        "# Call `seed_setter` to ensure reproducibility.\n",
        "seed_setter(seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko8kskcC9PVC"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Psmj-hddxrHV"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "mnist_data = fetch_openml(\"mnist_784\")\n",
        "x = mnist_data[\"data\"].astype('float').to_numpy()\n",
        "y = mnist_data[\"target\"].astype('int')\n",
        "\n",
        "# Normalize\n",
        "x /= 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "num_labels = len(np.unique(y))\n",
        "y_new = to_categorical(y, num_labels)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# In this section, split the data into training and test datasets, using 60,000 samples for training and the remaining samples for testing.\n",
        "train_size = 60000\n",
        "x_train = x[:train_size]\n",
        "y_train = y_new[:train_size]\n",
        "x_val = x[train_size:]\n",
        "y_val = y_new[train_size:]\n",
        "\n",
        "# ---------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "pG5s84eyyeh8",
        "outputId": "fe1b93ef-0600-4440-bfc0-8eca3f8ea503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data: (60000, 784) (60000, 10)\n",
            "Test data: (10000, 784) (10000, 10)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 750x400 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Training data: {} {}\".format(x_train.shape, y_train.shape))\n",
        "print(\"Test data: {} {}\".format(x_val.shape, y_val.shape))\n",
        "show_images(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0yB4KcT9ffj"
      },
      "source": [
        "# Activation Functions & Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFGJmu06sAAh"
      },
      "source": [
        "There are many activation functions which some of them listed below. In this section you should implement them.\n",
        "\n",
        "1. Sigmoid:\n",
        "$$\n",
        "\\operatorname{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}\n",
        "$$\n",
        "\n",
        "2. Softmax: $$\\operatorname{softmax}(\\mathbf X)_{ij} = \\frac{\\exp(\\mathbf X_{ij})}{\\sum_k \\exp \\mathbf X_{ik})}.$$\n",
        "\n",
        "\n",
        "3. Tanh (Hyperbolic Tangent):\n",
        "$$\n",
        "\\operatorname{tanh}(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}\n",
        "$$\n",
        "\n",
        "4. ReLU: $$\\operatorname{ReLU}(x) = \\max(0, x)$$\n",
        "\n",
        "5. Leaky ReLU:\n",
        "$$\n",
        "\\operatorname{LeakyReLU}(x) =\n",
        "\\begin{cases}\n",
        "x & \\text{if } x \\geq 0 \\\\\n",
        "\\alpha x & \\text{if } x < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "where \\( $\\alpha$ \\) is a small constant (e.g., 0.01).\n",
        "\n",
        "6. ELU (Exponential Linear Unit):\n",
        "$$\n",
        "\\operatorname{ELU}(x) =\n",
        "\\begin{cases}\n",
        "x & \\text{if } x > 0 \\\\\n",
        "\\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "where ($\\alpha$) is a positive constant.\n",
        "\n",
        "7. SELU (Scaled Exponential Linear Unit):\n",
        "$$\n",
        "\\operatorname{SELU}(x) =\n",
        "\\lambda\n",
        "\\begin{cases}\n",
        "x & \\text{if } x > 0 \\\\\n",
        "\\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "where \\( $\\lambda$ \\) and \\( $\\alpha$ \\) are predefined constants, typically \\( $\\lambda \\approx 1.0507$ \\) and \\( $\\alpha \\approx 1.67326$ \\).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_cQudDfrG7D"
      },
      "source": [
        "------\n",
        "There are several optimizer algorithms for optimizing the neural network over epochs. You should implement some of those mentioned in this section:\n",
        "\n",
        "***Stochastic Gradient Descent (SGD)***\n",
        "\n",
        "SGD is the basic form of gradient descent, where each weight (\\( w \\)) and bias (\\( b \\)) parameter is updated in the opposite direction of its gradient.\n",
        "\n",
        "**Update Rule:**\n",
        "$$\n",
        "w^{(t+1)} = w^{(t)} - \\eta \\cdot \\nabla_w L(w^{(t)}, b^{(t)})\n",
        "$$\n",
        "$$\n",
        "b^{(t+1)} = b^{(t)} - \\eta \\cdot \\nabla_b L(w^{(t)}, b^{(t)})\n",
        "$$\n",
        "\n",
        "- \\( w \\): Weight parameters\n",
        "- \\( b \\): Bias parameters\n",
        "- \\( $\\eta$ \\): Learning rate\n",
        "- \\( $\\nabla_w L(w, b)$ \\): Gradient of the loss with respect to \\( w \\)\n",
        "- \\( $\\nabla_b L(w, b)$ \\): Gradient of the loss with respect to \\( b \\)\n",
        "\n",
        "<br><br>\n",
        "\n",
        "***Momentum Optimizer***\n",
        "\n",
        "Momentum optimizer smooths parameter updates by adding a velocity term to accumulate past gradients.\n",
        "\n",
        "**Update Rule:**\n",
        "1. **Velocity Update for Weights and Biases:**\n",
        "   $$\n",
        "   v_w^{(t+1)} = \\beta \\cdot v_w^{(t)} + (1 - \\beta) \\cdot \\nabla_w L(w^{(t)}, b^{(t)})\n",
        "   $$\n",
        "   $$\n",
        "   v_b^{(t+1)} = \\beta \\cdot v_b^{(t)} + (1 - \\beta) \\cdot \\nabla_b L(w^{(t)}, b^{(t)})\n",
        "   $$\n",
        "\n",
        "2. **Parameter Update:**\n",
        "   $$\n",
        "   w^{(t+1)} = w^{(t)} - \\eta \\cdot v_w^{(t+1)}\n",
        "   $$\n",
        "   $$\n",
        "   b^{(t+1)} = b^{(t)} - \\eta \\cdot v_b^{(t+1)}\n",
        "   $$\n",
        "\n",
        "- \\( $v_w$ \\): Velocity term for weights\n",
        "- \\( $v_b$ \\): Velocity term for biases\n",
        "- \\( $\\beta$ \\): Momentum term, typically set to 0.9\n",
        "\n",
        "<br><br>\n",
        "\n",
        "***Adam Optimizer***\n",
        "\n",
        "Adam combines momentum and adaptive learning rates, using both first (momentum) and second (RMSprop) moment estimates.\n",
        "\n",
        "**Update Rule:**\n",
        "1. **Compute biased first and second moment estimates:**\n",
        "   $$\n",
        "   m_w^{(t+1)} = \\beta_1 \\cdot m_w^{(t)} + (1 - \\beta_1) \\cdot \\nabla_w L(w^{(t)}, b^{(t)})\n",
        "   $$\n",
        "   $$\n",
        "   m_b^{(t+1)} = \\beta_1 \\cdot m_b^{(t)} + (1 - \\beta_1) \\cdot \\nabla_b L(w^{(t)}, b^{(t)})\n",
        "   $$\n",
        "   $$\n",
        "   v_w^{(t+1)} = \\beta_2 \\cdot v_w^{(t)} + (1 - \\beta_2) \\cdot \\left( \\nabla_w L(w^{(t)}, b^{(t)}) \\right)^2\n",
        "   $$\n",
        "   $$\n",
        "   v_b^{(t+1)} = \\beta_2 \\cdot v_b^{(t)} + (1 - \\beta_2) \\cdot \\left( \\nabla_b L(w^{(t)}, b^{(t)}) \\right)^2\n",
        "   $$\n",
        "\n",
        "2. **Compute bias-corrected estimates:**\n",
        "   $$\n",
        "   \\hat{m}_w^{(t+1)} = \\frac{m_w^{(t+1)}}{1 - \\beta_1^{t+1}}, \\quad \\hat{m}_b^{(t+1)} = \\frac{m_b^{(t+1)}}{1 - \\beta_1^{t+1}}\n",
        "   $$\n",
        "   $$\n",
        "   \\hat{v}_w^{(t+1)} = \\frac{v_w^{(t+1)}}{1 - \\beta_2^{t+1}}, \\quad \\hat{v}_b^{(t+1)} = \\frac{v_b^{(t+1)}}{1 - \\beta_2^{t+1}}\n",
        "   $$\n",
        "\n",
        "3. **Parameter Update:**\n",
        "   $$\n",
        "   w^{(t+1)} = w^{(t)} - \\eta \\cdot \\frac{\\hat{m}_w^{(t+1)}}{\\sqrt{\\hat{v}_w^{(t+1)}} + \\epsilon}\n",
        "   $$\n",
        "   $$\n",
        "   b^{(t+1)} = b^{(t)} - \\eta \\cdot \\frac{\\hat{m}_b^{(t+1)}}{\\sqrt{\\hat{v}_b^{(t+1)}} + \\epsilon}\n",
        "   $$\n",
        "\n",
        "- \\( $m_w$ \\), \\( $m_b$ \\): First moment estimates (mean of gradients) for weights and biases\n",
        "- \\( $v_w$ \\), \\( $v_b$ \\): Second moment estimates (uncentered variance of gradients) for weights and biases\n",
        "- \\( $\\beta_1$ \\): Decay rate for first moment, typically 0.9\n",
        "- \\( $\\beta_2$ \\): Decay rate for second moment, typically 0.999\n",
        "- \\( $\\epsilon$ \\): Small constant for numerical stability (e.g., \\(10^{-8}\\))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83ucvivl9vec"
      },
      "source": [
        "# MLP from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FGfYG_DfJL9b"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, sizes, activation='relu'):\n",
        "        self.sizes = sizes\n",
        "        self.num_layers = len(sizes) - 1\n",
        "        self.activation_name = activation\n",
        "        self.params = self.initialize_weights()\n",
        "        self.cache = {}\n",
        "\n",
        "        # Activation functions\n",
        "        activations = {\n",
        "            'relu': self.relu,\n",
        "            'sigmoid': self.sigmoid,\n",
        "            'tanh': self.tanh,\n",
        "            'leaky_relu': self.leaky_relu,\n",
        "            'selu': self.selu,\n",
        "            'elu':self.elu,\n",
        "        }\n",
        "        self.activation = activations.get(activation)\n",
        "        if not self.activation:\n",
        "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        params = {}\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            input_size = self.sizes[i - 1]\n",
        "            output_size = self.sizes[i]\n",
        "            params[f\"W{i}\"] = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)\n",
        "            params[f\"b{i}\"] = np.zeros((output_size, 1))\n",
        "        return params\n",
        "\n",
        "    def relu(self, x, derivative=False):\n",
        "        if derivative:\n",
        "            return np.where(x > 0, 1, 0)\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def sigmoid(self, x, derivative=False):\n",
        "        sigmoid = 1 / (1 + np.exp(-x))\n",
        "        if derivative:\n",
        "            return sigmoid * (1 - sigmoid)\n",
        "        return sigmoid\n",
        "\n",
        "    def tanh(self, x, derivative=False):\n",
        "        tanh = np.tanh(x)\n",
        "        if derivative:\n",
        "            return 1 - tanh ** 2\n",
        "        return tanh\n",
        "\n",
        "    def leaky_relu(self, x, alpha=0.01, derivative=False):\n",
        "        if derivative:\n",
        "            return np.where(x > 0, 1, alpha)\n",
        "        return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "\n",
        "    def elu(self, x, alpha=1.0, derivative=False):\n",
        "        if derivative:\n",
        "            return np.where(x > 0, 1, alpha * np.exp(x))\n",
        "        return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
        "\n",
        "    def selu(self, x, derivative=False):\n",
        "        alpha = 1.67326\n",
        "        lambda_ = 1.0507\n",
        "        if derivative:\n",
        "            # Derivative of SELU\n",
        "            return lambda_ * (np.where(x > 0, 1, alpha * np.exp(x)))\n",
        "        return lambda_ * np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
        "\n",
        "    def feed_forward(self, x):\n",
        "        self.cache['A0'] = x.T\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            Z = np.dot(self.params[f\"W{i}\"], self.cache[f\"A{i-1}\"]) + self.params[f\"b{i}\"]\n",
        "            A = self.activation(Z) if i != self.num_layers else self.softmax(Z)\n",
        "            self.cache[f\"Z{i}\"] = Z\n",
        "            self.cache[f\"A{i}\"] = A\n",
        "        return self.cache[f\"A{self.num_layers}\"].T\n",
        "\n",
        "    def back_propagate(self, y, output):\n",
        "        m = y.shape[0]\n",
        "        grads = {}\n",
        "        dA = output.T - y.T  # Cross-entropy loss derivative\n",
        "\n",
        "        for i in reversed(range(1, self.num_layers + 1)):\n",
        "            dZ = dA if i == self.num_layers else dA * self.activation(self.cache[f\"Z{i}\"], derivative=True)\n",
        "            dW = np.dot(dZ, self.cache[f\"A{i-1}\"].T) / m\n",
        "            db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "            dA = np.dot(self.params[f\"W{i}\"].T, dZ) if i > 1 else None\n",
        "            grads[f\"W{i}\"] = dW\n",
        "            grads[f\"b{i}\"] = db\n",
        "        return grads\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exps = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
        "        return exps / np.sum(exps, axis=0, keepdims=True)\n",
        "\n",
        "    def cross_entropy_loss(self, y, output):\n",
        "        m = y.shape[0]\n",
        "        output = np.clip(output, 1e-9, 1 - 1e-9)\n",
        "        return -np.sum(y * np.log(output)) / m\n",
        "\n",
        "    def update_weights(self, grads, lr):\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            self.params[f\"W{i}\"] -= lr * grads[f\"W{i}\"]\n",
        "            self.params[f\"b{i}\"] -= lr * grads[f\"b{i}\"]\n",
        "\n",
        "    def accuracy(self, y, output):\n",
        "        predictions = np.argmax(output, axis=1)\n",
        "        labels = np.argmax(y, axis=1)\n",
        "        return np.mean(predictions == labels)\n",
        "\n",
        "    def train(self, x_train, y_train, x_val, y_val, epochs=100, batch_size=64, lr=0.01):\n",
        "        num_batches = -(-x_train.shape[0] // batch_size)\n",
        "        for epoch in range(epochs):\n",
        "            permutation = np.random.permutation(x_train.shape[0])\n",
        "            x_train_shuffled = x_train[permutation]\n",
        "            y_train_shuffled = y_train[permutation]\n",
        "\n",
        "            for batch in range(num_batches):\n",
        "                start = batch * batch_size\n",
        "                end = min(start + batch_size, x_train.shape[0])\n",
        "                x_batch = x_train_shuffled[start:end]\n",
        "                y_batch = y_train_shuffled[start:end]\n",
        "\n",
        "                output = self.feed_forward(x_batch)\n",
        "                grads = self.back_propagate(y_batch, output)\n",
        "                self.update_weights(grads, lr)\n",
        "\n",
        "            # Metrics\n",
        "            train_output = self.feed_forward(x_train)\n",
        "            train_loss = self.cross_entropy_loss(y_train, train_output)\n",
        "            train_acc = self.accuracy(y_train, train_output)\n",
        "            val_output = self.feed_forward(x_val)\n",
        "            val_loss = self.cross_entropy_loss(y_val, val_output)\n",
        "            val_acc = self.accuracy(y_val, val_output)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnSR8IkArgeT"
      },
      "source": [
        "Please run experiments with different combinations of optimizers (SGD, Momentum, and Adam) and activation functions (ReLU, Sigmoid, Tanh, Leaky ReLU, SELU, and ELU) over 20 epochs each. Consider networks with three layers (without input layer) whose number of neurons is 128, 64 and 10 respectively. Also consider two learning rates 0.01 and 0.001.\n",
        "\n",
        "\n",
        "After testing all combinations, report which combination of optimizer, activation function, and learning rate performs best.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zAksBBzrigw",
        "outputId": "7aca8b55-a0d5-43fc-9978-4e96982571dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15, Train Loss: 2.1605, Train Acc: 0.4345, Val Loss: 2.1575, Val Acc: 0.4379\n",
            "Epoch 2/15, Train Loss: 1.8956, Train Acc: 0.6077, Val Loss: 1.8881, Val Acc: 0.6054\n",
            "Epoch 3/15, Train Loss: 1.5193, Train Acc: 0.6818, Val Loss: 1.5078, Val Acc: 0.6847\n",
            "Epoch 4/15, Train Loss: 1.2023, Train Acc: 0.7402, Val Loss: 1.1905, Val Acc: 0.7429\n",
            "Epoch 5/15, Train Loss: 0.9804, Train Acc: 0.7891, Val Loss: 0.9679, Val Acc: 0.7929\n",
            "Epoch 6/15, Train Loss: 0.8287, Train Acc: 0.8079, Val Loss: 0.8155, Val Acc: 0.8140\n",
            "Epoch 7/15, Train Loss: 0.7220, Train Acc: 0.8304, Val Loss: 0.7075, Val Acc: 0.8360\n",
            "Epoch 8/15, Train Loss: 0.6441, Train Acc: 0.8453, Val Loss: 0.6290, Val Acc: 0.8513\n",
            "Epoch 9/15, Train Loss: 0.5853, Train Acc: 0.8557, Val Loss: 0.5699, Val Acc: 0.8626\n",
            "Epoch 10/15, Train Loss: 0.5399, Train Acc: 0.8640, Val Loss: 0.5243, Val Acc: 0.8710\n",
            "Epoch 11/15, Train Loss: 0.5041, Train Acc: 0.8708, Val Loss: 0.4886, Val Acc: 0.8782\n",
            "Epoch 12/15, Train Loss: 0.4750, Train Acc: 0.8773, Val Loss: 0.4592, Val Acc: 0.8857\n",
            "Epoch 13/15, Train Loss: 0.4518, Train Acc: 0.8810, Val Loss: 0.4363, Val Acc: 0.8882\n",
            "Epoch 14/15, Train Loss: 0.4314, Train Acc: 0.8854, Val Loss: 0.4165, Val Acc: 0.8922\n",
            "Epoch 15/15, Train Loss: 0.4149, Train Acc: 0.8891, Val Loss: 0.4000, Val Acc: 0.8953\n",
            "Trained with sigmoid activation, sgd optimizer, lr=0.01\n",
            "Epoch 1/15, Train Loss: 2.3010, Train Acc: 0.1615, Val Loss: 2.3010, Val Acc: 0.1596\n",
            "Epoch 2/15, Train Loss: 2.2853, Train Acc: 0.1638, Val Loss: 2.2849, Val Acc: 0.1615\n",
            "Epoch 3/15, Train Loss: 2.2711, Train Acc: 0.2073, Val Loss: 2.2704, Val Acc: 0.2069\n",
            "Epoch 4/15, Train Loss: 2.2566, Train Acc: 0.2374, Val Loss: 2.2556, Val Acc: 0.2398\n",
            "Epoch 5/15, Train Loss: 2.2417, Train Acc: 0.2934, Val Loss: 2.2403, Val Acc: 0.2964\n",
            "Epoch 6/15, Train Loss: 2.2262, Train Acc: 0.3105, Val Loss: 2.2244, Val Acc: 0.3129\n",
            "Epoch 7/15, Train Loss: 2.2097, Train Acc: 0.3777, Val Loss: 2.2075, Val Acc: 0.3845\n",
            "Epoch 8/15, Train Loss: 2.1924, Train Acc: 0.3931, Val Loss: 2.1898, Val Acc: 0.3989\n",
            "Epoch 9/15, Train Loss: 2.1739, Train Acc: 0.4180, Val Loss: 2.1709, Val Acc: 0.4251\n",
            "Epoch 10/15, Train Loss: 2.1542, Train Acc: 0.4434, Val Loss: 2.1507, Val Acc: 0.4524\n",
            "Epoch 11/15, Train Loss: 2.1332, Train Acc: 0.4766, Val Loss: 2.1292, Val Acc: 0.4846\n",
            "Epoch 12/15, Train Loss: 2.1107, Train Acc: 0.4871, Val Loss: 2.1062, Val Acc: 0.4943\n",
            "Epoch 13/15, Train Loss: 2.0868, Train Acc: 0.4933, Val Loss: 2.0817, Val Acc: 0.5012\n",
            "Epoch 14/15, Train Loss: 2.0612, Train Acc: 0.5230, Val Loss: 2.0555, Val Acc: 0.5322\n",
            "Epoch 15/15, Train Loss: 2.0339, Train Acc: 0.5388, Val Loss: 2.0278, Val Acc: 0.5495\n",
            "Trained with sigmoid activation, sgd optimizer, lr=0.001\n",
            "Epoch 1/15, Train Loss: 2.1342, Train Acc: 0.5103, Val Loss: 2.1292, Val Acc: 0.5282\n",
            "Epoch 2/15, Train Loss: 1.8410, Train Acc: 0.6807, Val Loss: 1.8299, Val Acc: 0.6929\n",
            "Epoch 3/15, Train Loss: 1.4372, Train Acc: 0.7206, Val Loss: 1.4211, Val Acc: 0.7310\n",
            "Epoch 4/15, Train Loss: 1.1094, Train Acc: 0.7641, Val Loss: 1.0928, Val Acc: 0.7693\n",
            "Epoch 5/15, Train Loss: 0.8983, Train Acc: 0.7943, Val Loss: 0.8814, Val Acc: 0.7981\n",
            "Epoch 6/15, Train Loss: 0.7608, Train Acc: 0.8250, Val Loss: 0.7439, Val Acc: 0.8258\n",
            "Epoch 7/15, Train Loss: 0.6672, Train Acc: 0.8398, Val Loss: 0.6499, Val Acc: 0.8434\n",
            "Epoch 8/15, Train Loss: 0.6014, Train Acc: 0.8513, Val Loss: 0.5839, Val Acc: 0.8531\n",
            "Epoch 9/15, Train Loss: 0.5524, Train Acc: 0.8592, Val Loss: 0.5351, Val Acc: 0.8627\n",
            "Epoch 10/15, Train Loss: 0.5150, Train Acc: 0.8677, Val Loss: 0.4978, Val Acc: 0.8724\n",
            "Epoch 11/15, Train Loss: 0.4852, Train Acc: 0.8734, Val Loss: 0.4684, Val Acc: 0.8769\n",
            "Epoch 12/15, Train Loss: 0.4613, Train Acc: 0.8787, Val Loss: 0.4449, Val Acc: 0.8822\n",
            "Epoch 13/15, Train Loss: 0.4412, Train Acc: 0.8825, Val Loss: 0.4252, Val Acc: 0.8868\n",
            "Epoch 14/15, Train Loss: 0.4244, Train Acc: 0.8858, Val Loss: 0.4086, Val Acc: 0.8893\n",
            "Epoch 15/15, Train Loss: 0.4102, Train Acc: 0.8888, Val Loss: 0.3947, Val Acc: 0.8920\n",
            "Trained with sigmoid activation, momentum optimizer, lr=0.01\n",
            "Epoch 1/15, Train Loss: 2.2847, Train Acc: 0.1664, Val Loss: 2.2853, Val Acc: 0.1729\n",
            "Epoch 2/15, Train Loss: 2.2639, Train Acc: 0.1999, Val Loss: 2.2631, Val Acc: 0.2085\n",
            "Epoch 3/15, Train Loss: 2.2480, Train Acc: 0.2831, Val Loss: 2.2467, Val Acc: 0.2940\n",
            "Epoch 4/15, Train Loss: 2.2319, Train Acc: 0.3065, Val Loss: 2.2301, Val Acc: 0.3181\n",
            "Epoch 5/15, Train Loss: 2.2154, Train Acc: 0.3699, Val Loss: 2.2133, Val Acc: 0.3833\n",
            "Epoch 6/15, Train Loss: 2.1982, Train Acc: 0.4052, Val Loss: 2.1957, Val Acc: 0.4199\n",
            "Epoch 7/15, Train Loss: 2.1803, Train Acc: 0.4589, Val Loss: 2.1773, Val Acc: 0.4696\n",
            "Epoch 8/15, Train Loss: 2.1614, Train Acc: 0.4767, Val Loss: 2.1580, Val Acc: 0.4854\n",
            "Epoch 9/15, Train Loss: 2.1415, Train Acc: 0.5036, Val Loss: 2.1376, Val Acc: 0.5116\n",
            "Epoch 10/15, Train Loss: 2.1203, Train Acc: 0.5312, Val Loss: 2.1159, Val Acc: 0.5387\n",
            "Epoch 11/15, Train Loss: 2.0979, Train Acc: 0.5391, Val Loss: 2.0929, Val Acc: 0.5498\n",
            "Epoch 12/15, Train Loss: 2.0740, Train Acc: 0.5689, Val Loss: 2.0684, Val Acc: 0.5770\n",
            "Epoch 13/15, Train Loss: 2.0486, Train Acc: 0.5768, Val Loss: 2.0424, Val Acc: 0.5861\n",
            "Epoch 14/15, Train Loss: 2.0216, Train Acc: 0.5901, Val Loss: 2.0148, Val Acc: 0.6000\n",
            "Epoch 15/15, Train Loss: 1.9930, Train Acc: 0.5984, Val Loss: 1.9856, Val Acc: 0.6075\n",
            "Trained with sigmoid activation, momentum optimizer, lr=0.001\n",
            "Epoch 1/15, Train Loss: 2.1244, Train Acc: 0.4821, Val Loss: 2.1207, Val Acc: 0.4943\n",
            "Epoch 2/15, Train Loss: 1.8163, Train Acc: 0.5997, Val Loss: 1.8069, Val Acc: 0.6030\n",
            "Epoch 3/15, Train Loss: 1.4390, Train Acc: 0.6725, Val Loss: 1.4260, Val Acc: 0.6822\n",
            "Epoch 4/15, Train Loss: 1.1571, Train Acc: 0.7365, Val Loss: 1.1431, Val Acc: 0.7453\n",
            "Epoch 5/15, Train Loss: 0.9675, Train Acc: 0.7698, Val Loss: 0.9524, Val Acc: 0.7794\n",
            "Epoch 6/15, Train Loss: 0.8344, Train Acc: 0.7981, Val Loss: 0.8191, Val Acc: 0.8055\n",
            "Epoch 7/15, Train Loss: 0.7367, Train Acc: 0.8212, Val Loss: 0.7201, Val Acc: 0.8260\n",
            "Epoch 8/15, Train Loss: 0.6628, Train Acc: 0.8340, Val Loss: 0.6463, Val Acc: 0.8395\n",
            "Epoch 9/15, Train Loss: 0.6062, Train Acc: 0.8451, Val Loss: 0.5890, Val Acc: 0.8502\n",
            "Epoch 10/15, Train Loss: 0.5608, Train Acc: 0.8560, Val Loss: 0.5436, Val Acc: 0.8619\n",
            "Epoch 11/15, Train Loss: 0.5243, Train Acc: 0.8641, Val Loss: 0.5078, Val Acc: 0.8692\n",
            "Epoch 12/15, Train Loss: 0.4944, Train Acc: 0.8716, Val Loss: 0.4776, Val Acc: 0.8758\n",
            "Epoch 13/15, Train Loss: 0.4680, Train Acc: 0.8781, Val Loss: 0.4516, Val Acc: 0.8810\n",
            "Epoch 14/15, Train Loss: 0.4462, Train Acc: 0.8828, Val Loss: 0.4301, Val Acc: 0.8860\n",
            "Epoch 15/15, Train Loss: 0.4273, Train Acc: 0.8865, Val Loss: 0.4123, Val Acc: 0.8904\n",
            "Trained with sigmoid activation, adam optimizer, lr=0.01\n",
            "Epoch 1/15, Train Loss: 2.2936, Train Acc: 0.1033, Val Loss: 2.2912, Val Acc: 0.1031\n",
            "Epoch 2/15, Train Loss: 2.2740, Train Acc: 0.1339, Val Loss: 2.2719, Val Acc: 0.1341\n",
            "Epoch 3/15, Train Loss: 2.2606, Train Acc: 0.1989, Val Loss: 2.2584, Val Acc: 0.2079\n",
            "Epoch 4/15, Train Loss: 2.2471, Train Acc: 0.2396, Val Loss: 2.2445, Val Acc: 0.2506\n",
            "Epoch 5/15, Train Loss: 2.2332, Train Acc: 0.3547, Val Loss: 2.2303, Val Acc: 0.3735\n",
            "Epoch 6/15, Train Loss: 2.2189, Train Acc: 0.3785, Val Loss: 2.2158, Val Acc: 0.4002\n",
            "Epoch 7/15, Train Loss: 2.2038, Train Acc: 0.4232, Val Loss: 2.2004, Val Acc: 0.4421\n",
            "Epoch 8/15, Train Loss: 2.1881, Train Acc: 0.4599, Val Loss: 2.1843, Val Acc: 0.4774\n",
            "Epoch 9/15, Train Loss: 2.1714, Train Acc: 0.4832, Val Loss: 2.1672, Val Acc: 0.5029\n",
            "Epoch 10/15, Train Loss: 2.1537, Train Acc: 0.5001, Val Loss: 2.1492, Val Acc: 0.5183\n",
            "Epoch 11/15, Train Loss: 2.1348, Train Acc: 0.5434, Val Loss: 2.1299, Val Acc: 0.5594\n",
            "Epoch 12/15, Train Loss: 2.1147, Train Acc: 0.5445, Val Loss: 2.1093, Val Acc: 0.5576\n",
            "Epoch 13/15, Train Loss: 2.0931, Train Acc: 0.5549, Val Loss: 2.0872, Val Acc: 0.5657\n",
            "Epoch 14/15, Train Loss: 2.0699, Train Acc: 0.5759, Val Loss: 2.0636, Val Acc: 0.5853\n",
            "Epoch 15/15, Train Loss: 2.0451, Train Acc: 0.5878, Val Loss: 2.0384, Val Acc: 0.5974\n",
            "Trained with sigmoid activation, adam optimizer, lr=0.001\n",
            "Epoch 1/15, Train Loss: 0.3883, Train Acc: 0.8932, Val Loss: 0.3707, Val Acc: 0.8982\n",
            "Epoch 2/15, Train Loss: 0.3003, Train Acc: 0.9152, Val Loss: 0.2908, Val Acc: 0.9177\n",
            "Epoch 3/15, Train Loss: 0.2565, Train Acc: 0.9258, Val Loss: 0.2503, Val Acc: 0.9282\n",
            "Epoch 4/15, Train Loss: 0.2298, Train Acc: 0.9340, Val Loss: 0.2290, Val Acc: 0.9357\n",
            "Epoch 5/15, Train Loss: 0.2078, Train Acc: 0.9414, Val Loss: 0.2083, Val Acc: 0.9406\n",
            "Epoch 6/15, Train Loss: 0.1907, Train Acc: 0.9453, Val Loss: 0.1935, Val Acc: 0.9425\n",
            "Epoch 7/15, Train Loss: 0.1788, Train Acc: 0.9496, Val Loss: 0.1818, Val Acc: 0.9463\n",
            "Epoch 8/15, Train Loss: 0.1666, Train Acc: 0.9522, Val Loss: 0.1715, Val Acc: 0.9495\n",
            "Epoch 9/15, Train Loss: 0.1559, Train Acc: 0.9554, Val Loss: 0.1628, Val Acc: 0.9518\n",
            "Epoch 10/15, Train Loss: 0.1472, Train Acc: 0.9582, Val Loss: 0.1548, Val Acc: 0.9550\n",
            "Epoch 11/15, Train Loss: 0.1394, Train Acc: 0.9599, Val Loss: 0.1477, Val Acc: 0.9578\n",
            "Epoch 12/15, Train Loss: 0.1345, Train Acc: 0.9620, Val Loss: 0.1450, Val Acc: 0.9570\n",
            "Epoch 13/15, Train Loss: 0.1255, Train Acc: 0.9647, Val Loss: 0.1369, Val Acc: 0.9608\n",
            "Epoch 14/15, Train Loss: 0.1211, Train Acc: 0.9660, Val Loss: 0.1320, Val Acc: 0.9621\n",
            "Epoch 15/15, Train Loss: 0.1135, Train Acc: 0.9686, Val Loss: 0.1248, Val Acc: 0.9638\n",
            "Trained with relu activation, sgd optimizer, lr=0.01\n",
            "Epoch 1/15, Train Loss: 1.6490, Train Acc: 0.6048, Val Loss: 1.6313, Val Acc: 0.6154\n",
            "Epoch 2/15, Train Loss: 1.0753, Train Acc: 0.7715, Val Loss: 1.0473, Val Acc: 0.7813\n",
            "Epoch 3/15, Train Loss: 0.7894, Train Acc: 0.8173, Val Loss: 0.7604, Val Acc: 0.8304\n",
            "Epoch 4/15, Train Loss: 0.6465, Train Acc: 0.8416, Val Loss: 0.6195, Val Acc: 0.8532\n",
            "Epoch 5/15, Train Loss: 0.5636, Train Acc: 0.8559, Val Loss: 0.5385, Val Acc: 0.8647\n",
            "Epoch 6/15, Train Loss: 0.5096, Train Acc: 0.8665, Val Loss: 0.4860, Val Acc: 0.8760\n",
            "Epoch 7/15, Train Loss: 0.4709, Train Acc: 0.8743, Val Loss: 0.4488, Val Acc: 0.8828\n",
            "Epoch 8/15, Train Loss: 0.4421, Train Acc: 0.8815, Val Loss: 0.4215, Val Acc: 0.8858\n",
            "Epoch 9/15, Train Loss: 0.4190, Train Acc: 0.8863, Val Loss: 0.3998, Val Acc: 0.8919\n",
            "Epoch 10/15, Train Loss: 0.4006, Train Acc: 0.8902, Val Loss: 0.3827, Val Acc: 0.8964\n",
            "Epoch 11/15, Train Loss: 0.3849, Train Acc: 0.8937, Val Loss: 0.3677, Val Acc: 0.8990\n",
            "Epoch 12/15, Train Loss: 0.3717, Train Acc: 0.8965, Val Loss: 0.3555, Val Acc: 0.9012\n",
            "Epoch 13/15, Train Loss: 0.3605, Train Acc: 0.8990, Val Loss: 0.3454, Val Acc: 0.9031\n",
            "Epoch 14/15, Train Loss: 0.3505, Train Acc: 0.9019, Val Loss: 0.3358, Val Acc: 0.9059\n",
            "Epoch 15/15, Train Loss: 0.3414, Train Acc: 0.9038, Val Loss: 0.3275, Val Acc: 0.9071\n",
            "Trained with relu activation, sgd optimizer, lr=0.001\n",
            "Epoch 1/15, Train Loss: 0.3973, Train Acc: 0.8888, Val Loss: 0.3813, Val Acc: 0.8944\n"
          ]
        }
      ],
      "source": [
        "sizes = [784, 128, 64, 10]\n",
        "\n",
        "activation_functions = ['sigmoid', 'relu', 'tanh', 'leaky_relu', 'selu', 'elu']\n",
        "optimizers = ['sgd', 'momentum', 'adam']\n",
        "learning_rates = [0.01, 0.001]\n",
        "\n",
        "best_model = None\n",
        "best_accuracy = 0.0\n",
        "best_config = None\n",
        "\n",
        "for activation in activation_functions:\n",
        "    for optimizer in optimizers:\n",
        "        for lr in learning_rates:\n",
        "            mlp = MLP(sizes, activation)\n",
        "            mlp.optimizer = optimizer\n",
        "            mlp.train(x_train, y_train, x_val, y_val, epochs=15, batch_size=64, lr=lr)\n",
        "            print(f\"Trained with {activation} activation, {optimizer} optimizer, lr={lr}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABnVeI2Gx2nk"
      },
      "source": [
        "# MLP as function Approximator\n",
        "We know that a MLP with one hidden layer could  approximate any smooth function! <br> Here you will manually fit a sine function using ReLU activation. <br>You need to set the correct weights on ReLUs so the linear combination approximates the desired function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F__DcOEz2y1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def relu_approximation(input_data, target_labels):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      input_data: torch.tensor\n",
        "        Input training data (1D tensor)\n",
        "      target_labels: torch.tensor\n",
        "        Ground truth labels for training data (1D tensor)\n",
        "\n",
        "    Returns:\n",
        "      predicted_labels: torch.tensor\n",
        "        Estimated labels or class predictions\n",
        "        Derived from weighted sum of ReLU activations along the x-axis\n",
        "      activation_map: torch.tensor\n",
        "        ReLU activations computed across the x-axis\n",
        "      x_points: torch.tensor\n",
        "        Points along the x-axis\n",
        "    \"\"\"\n",
        "    num_relus = input_data.shape[0] - 1\n",
        "\n",
        "    x_points = torch.linspace(torch.min(input_data), torch.max(input_data), 1000)\n",
        "\n",
        "    biases = torch.linspace(torch.min(input_data), torch.max(input_data), num_relus)\n",
        "\n",
        "    activation_map = torch.zeros((num_relus, len(x_points)))\n",
        "\n",
        "    for relu_idx in range(num_relus):\n",
        "        activation_map[relu_idx, :] = torch.relu(x_points - biases[relu_idx])\n",
        "\n",
        "\n",
        "    activation_map_training = torch.zeros((num_relus, len(input_data)))\n",
        "    for relu_idx in range(num_relus):\n",
        "        activation_map_training[relu_idx, :] = torch.relu(input_data.flatten() - biases[relu_idx])\n",
        "\n",
        "    # Solve the least squares problem\n",
        "    relu_weights = torch.linalg.lstsq(activation_map_training.T, target_labels).solution\n",
        "\n",
        "    # Use activation map to compute predictions\n",
        "    predicted_labels = (activation_map.T @ relu_weights).flatten()  # Change to activation_map.T for correct shape\n",
        "\n",
        "    return predicted_labels, activation_map, x_points\n",
        "\n",
        "\n",
        "def plot_function_apx(x_points, activation_map, predicted_output, input_data, target_labels):\n",
        "    \"\"\"Plot the true sine function, ReLU approximation, and individual ReLU activations.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.plot(input_data, target_labels, label=\"True Sine Function\", color=\"blue\", linewidth=2)\n",
        "\n",
        "    plt.plot(x_points, predicted_output.detach().numpy(), label=\"ReLU Approximation\", color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    for i in range(activation_map.shape[0]):\n",
        "        plt.plot(x_points, activation_map[i, :].detach().numpy(), linestyle=\":\", label=f\"ReLU {i+1}\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title(\"Sine Function Approximation Using ReLU\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"f(x)\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "num_samples = 10\n",
        "input_data = torch.linspace(0, 2 * np.pi, num_samples).view(-1, 1)\n",
        "target_labels = torch.sin(input_data)\n",
        "\n",
        "predicted_output, activation_map, x_points = relu_approximation(input_data, target_labels)\n",
        "\n",
        "plot_function_apx(x_points, activation_map, predicted_output, input_data, target_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPk-_Xll83dA"
      },
      "source": [
        "# Depth vs. Width\n",
        "Here we wanna see the effect of width vs. depth on a classification task.First you are going to implement MLP again; But with 2 differences: <br>\n",
        "\n",
        "1.   It is general purpose (i.e. works for desired depth and activation functions)\n",
        "2.   You will implement it using pytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjigdUhZw_em"
      },
      "outputs": [],
      "source": [
        "class MLP_pytorch(nn.Module):\n",
        "    \"\"\"\n",
        "    Simulate MLP Network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, activation_fn, input_feature_num, hidden_unit_nums, output_feature_num):\n",
        "        \"\"\"\n",
        "        Initialize MLP Network parameters\n",
        "\n",
        "        Args:\n",
        "          activation_fn: string\n",
        "            Activation function ('ReLU', 'Tanh', 'Sigmoid')\n",
        "          input_feature_num: int\n",
        "            Number of input features\n",
        "          hidden_unit_nums: list\n",
        "            Number of units per hidden layer. List of integers\n",
        "          output_feature_num: int\n",
        "            Number of output features\n",
        "\n",
        "        Returns:\n",
        "          Nothing\n",
        "        \"\"\"\n",
        "        super(MLP_pytorch, self).__init__()\n",
        "        self.input_feature_num = input_feature_num\n",
        "        self.mlp = nn.Sequential()\n",
        "\n",
        "        in_num = input_feature_num\n",
        "\n",
        "        # Add hidden layers with activation functions\n",
        "        for i, out_num in enumerate(hidden_unit_nums):\n",
        "            self.mlp.add_module(f'Linear_{i}', nn.Linear(in_num, out_num))\n",
        "\n",
        "            # Add activation function based on the input argument\n",
        "            if activation_fn == 'ReLU':\n",
        "                self.mlp.add_module(f'ReLU_{i}', nn.ReLU())\n",
        "            elif activation_fn == 'Tanh':\n",
        "                self.mlp.add_module(f'Tanh_{i}', nn.Tanh())\n",
        "            elif activation_fn == 'Sigmoid':\n",
        "                self.mlp.add_module(f'Sigmoid_{i}', nn.Sigmoid())\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported activation function: {activation_fn}\")\n",
        "\n",
        "            in_num = out_num\n",
        "\n",
        "        # Add the output layer\n",
        "        out_layer = nn.Linear(in_num, output_feature_num)\n",
        "        self.mlp.add_module('Output_Linear', out_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Simulate forward pass of MLP Network\n",
        "\n",
        "        Args:\n",
        "          x: torch.tensor\n",
        "            Input data\n",
        "\n",
        "        Returns:\n",
        "          logits: torch.tensor\n",
        "            Output after forward pass\n",
        "        \"\"\"\n",
        "        # Reshape inputs to (batch_size, input_feature_num)\n",
        "        x = x.view(-1, self.input_feature_num)\n",
        "        logits = self.mlp(x)  # Forward pass\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ali1hMF6vpY5"
      },
      "source": [
        "Now let's make a spiral dataset that follows this formula:\n",
        "\\begin{equation}\n",
        "\\begin{array}{c}\n",
        "X_{k}(t)=t\\left(\\begin{array}{c}\n",
        "\\sin \\left[\\frac{2 \\pi}{K}\\left(2 t+k-1\\right)\\right]+\\mathcal{N}\\left(0, \\sigma\\right) \\\\\n",
        "\\cos \\left[\\frac{2 \\pi}{K}\\left(2 t+k-1\\right)\\right]+\\mathcal{N}\\left(0, \\sigma\\right)\n",
        "\\end{array}\\right)\n",
        "\\end{array}, \\quad 0 \\leq t \\leq 1, \\quad k=1, \\ldots, K\n",
        "\\end{equation}\n",
        "\n",
        "Run cell below to create the data and load it as tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-f3enyo551x5"
      },
      "outputs": [],
      "source": [
        "# @title Data Loader\n",
        "K = 4\n",
        "sigma = 0.4\n",
        "N = 1000\n",
        "t = torch.linspace(0, 1, N)\n",
        "X = torch.zeros(K*N, 2)\n",
        "y = torch.zeros(K*N)\n",
        "for k in range(K):\n",
        "  X[k*N:(k+1)*N, 0] = t*(torch.sin(2*np.pi/K*(2*t+k)) + sigma**2*torch.randn(N))\n",
        "  X[k*N:(k+1)*N, 1] = t*(torch.cos(2*np.pi/K*(2*t+k)) + sigma**2*torch.randn(N))\n",
        "  y[k*N:(k+1)*N] = k\n",
        "\n",
        "\n",
        "X_test, y_test, X_train, y_train = shuffle_and_split_data(X, y, seed=SEED)\n",
        "\n",
        "# DataLoader with random seed\n",
        "batch_size = 128\n",
        "g_seed = torch.Generator()\n",
        "g_seed.manual_seed(SEED)\n",
        "\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size,\n",
        "                         shuffle=False, num_workers=0,\n",
        "                         worker_init_fn=seed_worker,\n",
        "                         generator=g_seed,\n",
        "                         )\n",
        "\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=batch_size,\n",
        "                          drop_last=True,\n",
        "                          shuffle=True,\n",
        "                          worker_init_fn=seed_worker,\n",
        "                          generator=g_seed,\n",
        "                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1REALnz4Jw_"
      },
      "source": [
        "Now we will add more polynomial features to the dataset to make the first layer wider. Afterwards, train a single linear layer. We could use the same MLP network with no hidden layers (though it would not be called an MLP anymore!).\n",
        "\n",
        "Add polynomial terms up to $P=50$ which means that for every $x_1^n x_2^m$ term, $n+m\\leq P$. Total number of polynomial features up to $P$ follows this formula:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{# of terms} = \\frac{(P+1)(P+2)}{2}\n",
        "\\end{equation}\n",
        "\n",
        "Also, we don't need the polynomial term with degree zero (which is the constatnt term) since `nn.Linear` layers have bias terms. Therefore we will have one fewer polynomial feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtuvmsKf37jg"
      },
      "outputs": [],
      "source": [
        "def polynomial_classifier(poly_degree, seed=0):\n",
        "    \"\"\"\n",
        "    Helper function to run the polynomial classifier\n",
        "\n",
        "    Args:\n",
        "      poly_degree: int\n",
        "        Degree of the polynomial\n",
        "      seed: int\n",
        "        A non-negative integer that defines the random state.\n",
        "\n",
        "    Returns:\n",
        "      num_features: int\n",
        "        Number of features\n",
        "    \"\"\"\n",
        "\n",
        "    def polynomial_features(poly_degree, X):\n",
        "        \"\"\"\n",
        "        Function to define the number of polynomial features except the bias term\n",
        "\n",
        "        Args:\n",
        "          poly_degree: int\n",
        "            Degree of the polynomial\n",
        "          X: torch.tensor\n",
        "            Input data\n",
        "\n",
        "        Returns:\n",
        "          poly_X: torch.tensor\n",
        "            Polynomial terms\n",
        "          num_features: int\n",
        "            Number of features\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        poly_X = []\n",
        "        for degree in range(1, poly_degree + 1):\n",
        "            for combo in combinations_with_replacement(range(n_features), degree):\n",
        "                poly_X.append(torch.prod(X[:, combo], dim=1).view(-1, 1))\n",
        "\n",
        "        poly_X = torch.cat(poly_X, dim=1)\n",
        "        num_features = poly_X.shape[1]\n",
        "\n",
        "        return poly_X, num_features\n",
        "\n",
        "    ############################################################\n",
        "    poly_X_test, num_features = polynomial_features(poly_degree, X_test)\n",
        "    poly_X_train, _ = polynomial_features(poly_degree, X_train)\n",
        "\n",
        "    batch_size = 128\n",
        "    g_seed = torch.Generator()\n",
        "    g_seed.manual_seed(seed)\n",
        "    poly_test_data = TensorDataset(poly_X_test, y_test)\n",
        "    poly_test_loader = DataLoader(poly_test_data,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=False,\n",
        "                                  num_workers=1,\n",
        "                                  worker_init_fn=seed_worker,\n",
        "                                  generator=g_seed)\n",
        "\n",
        "    poly_train_data = TensorDataset(poly_X_train, y_train)\n",
        "    poly_train_loader = DataLoader(poly_train_data,\n",
        "                                   batch_size=batch_size,\n",
        "                                   shuffle=True,\n",
        "                                   num_workers=1,\n",
        "                                   worker_init_fn=seed_worker,\n",
        "                                   generator=g_seed)\n",
        "\n",
        "    ############################################################\n",
        "    poly_net = MLP_pytorch('relu', num_features, [], K)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(poly_net.parameters(), lr=1e-3)\n",
        "    _, _ = train_test_classification(poly_net, criterion, optimizer,\n",
        "                                     poly_train_loader, poly_test_loader,\n",
        "                                     num_epochs=100)\n",
        "\n",
        "    X_all = sample_grid()\n",
        "    poly_X_all, _ = polynomial_features(poly_degree, X_all)\n",
        "    y_pred = poly_net(poly_X_all)\n",
        "\n",
        "    plot_decision_map(X_all.cpu(), y_pred.cpu(), X_test.cpu(), y_test.cpu())\n",
        "    plt.show()\n",
        "\n",
        "    return num_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pp3bFmq7JcC"
      },
      "source": [
        "### Train the network. How does it generalize?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW3_-gzM7W3d"
      },
      "outputs": [],
      "source": [
        "seed_setter(seed=SEED)\n",
        "max_poly_deg = 50\n",
        "num_features = polynomial_classifier(max_poly_deg)\n",
        "print(f'Number of features: {num_features}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz_CvYoQ_lit"
      },
      "source": [
        "Now create another instance of `MLP_pytorch` class having a hidden layer of 128 neurons and train it. Compare the result with the wide network. How does deeper model generalize? Is the decision boundaries ideal? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP4lOtyp6Zcz"
      },
      "outputs": [],
      "source": [
        "##############################----##############################\n",
        "seed_setter(SEED)\n",
        "net2 = MLP_pytorch('ReLU', X.shape[1], [128], K)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net2.parameters(), lr=1e-3)\n",
        "num_epochs = 100\n",
        "\n",
        "_, _ = train_test_classification(net2, criterion, optimizer, train_loader,\n",
        "                                 test_loader, num_epochs=num_epochs,\n",
        "                                 training_plot=True)\n",
        "\n",
        "X_all = sample_grid()\n",
        "y_pred = net2(X_all).cpu()\n",
        "plot_decision_map(X_all, y_pred, X_test, y_test)\n",
        "\n",
        "##############################------##############################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
